import requests
import json
import os
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
load_dotenv()

def chat_with_ollama(model_name, prompt, stream=False):
    """
    Call Ollama API to chat with a model

    Args:
        model_name: Name of the model (e.g., 'huihui_ai/qwen3-abliterated:30b')
        prompt: The prompt/question to send to the model
        stream: Whether to stream the response

    Returns:
        The model's response
    """
    url = "http://localhost:11434/api/generate"

    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": stream
    }

    response = requests.post(url, json=payload)

    if stream:
        # Handle streaming response
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_response = json.loads(line)
                if 'response' in json_response:
                    chunk = json_response['response']
                    print(chunk, end='', flush=True)
                    full_response += chunk
        print()  # New line at the end
        return full_response
    else:
        # Handle non-streaming response
        result = response.json()
        return result.get('response', '')


def chat_conversation(model_name, messages, stream=False, callback=None):
    """
    Have a conversation using the chat endpoint

    Args:
        model_name: Name of the model
        messages: List of message dicts with 'role' and 'content'
        stream: Whether to stream the response
        callback: Function to call for each chunk (only used when stream=True)

    Returns:
        The assistant's response
    """
    url = "http://localhost:11434/api/chat"

    payload = {
        "model": model_name,
        "messages": messages,
        "stream": stream
    }

    response = requests.post(url, json=payload, stream=stream)

    if stream:
        # Handle streaming response
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_response = json.loads(line)
                if 'message' in json_response and 'content' in json_response['message']:
                    chunk = json_response['message']['content']
                    full_response += chunk
                    if callback:
                        callback(chunk)
                    else:
                        print(chunk, end='', flush=True)
        if not callback:
            print()  # New line at the end
        return full_response
    else:
        result = response.json()
        return result.get('message', {}).get('content', '')


def chat_with_gemini(messages, stream=False, callback=None):
    """
    Have a conversation using Gemini API via OpenAI-compatible endpoint

    Args:
        messages: List of message dicts with 'role' and 'content'
        stream: Whether to stream the response
        callback: Function to call for each chunk (only used when stream=True)

    Returns:
        The assistant's response
    """
    api_key = os.getenv('GEMINI_API_KEY')
    model_name = os.getenv('GEMINI_MODEL', 'gemini-2.0-flash-exp')
    base_url = os.getenv('GEMINI_BASE_URL', 'https://api.laozhang.ai/v1/')

    client = OpenAI(
        api_key=api_key,
        base_url=base_url
    )

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            stream=stream
        )

        if stream:
            full_response = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    if callback:
                        callback(content)
                    else:
                        print(content, end='', flush=True)
            if not callback:
                print()  # New line at the end
            return full_response
        else:
            return response.choices[0].message.content
    except Exception as e:
        print(f"âŒ Gemini API é”™è¯¯: {str(e)}", flush=True)
        raise

# æç¤ºè¯ç¼“å­˜
_prompt_cache = {}

def generate_prompt(user_req: str, stream=False, log_callback=None):
    """
    ç”Ÿæˆæç¤ºè¯ï¼Œå¸¦ç¼“å­˜æœºåˆ¶

    Args:
        user_req: ç”¨æˆ·éœ€æ±‚æè¿°
        stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°ï¼Œç”¨äºå®æ—¶è¾“å‡ºæ—¥å¿—

    Returns:
        (positive_prompt, negative_prompt) å…ƒç»„
    """
    def log(msg):
        """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºå‡½æ•°"""
        print(msg, flush=True)
        if log_callback:
            log_callback(msg)

    # æ£€æŸ¥ç¼“å­˜
    if user_req in _prompt_cache:
        log(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æç¤ºè¯ï¼ˆç”¨æˆ·éœ€æ±‚: {user_req[:30]}...ï¼‰")
        return _prompt_cache[user_req]

    # è·å– AI Provider é…ç½®
    provider = os.getenv('AI_PROVIDER', 'ollama').lower()
    log(f"ğŸ¤– ä½¿ç”¨ AI Provider: {provider}")

    # ç”Ÿæˆæ–°çš„æç¤ºè¯
    log(f"ğŸ”„ ç”Ÿæˆæ–°çš„æç¤ºè¯ï¼ˆç”¨æˆ·éœ€æ±‚: {user_req[:30]}...ï¼‰")

    messages = [
        {"role": "system", "content": """
# role
ä½ æ˜¯ä¸€ä¸ªcomfyuiçš„æç¤ºè¯è®¾è®¡å¤§å¸ˆï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è®¾è®¡ç¬¦åˆcomfyuiè¦æ±‚çš„æç¤ºè¯

# æ­£é¢-è´Ÿé¢æç¤ºè¯æ ·ä¾‹
## group1
- (masterpiece,best quality,score_9, score_8_up, score_7_up:1),RAW,dynamic angle, sitting on a park bench,(night:1.5), 1girl,long hair,hair,make up,red lipstick,black eyes,air bangs,large breasts,bracelet,round earrings, blush,sexy,pose,posing,closeup,selfie, nipples,areolas,south korea idol,lace (see-through dress),necklace,jewelry,shoulder strap dress,breasts out,depth of field,bokeh
- (score_6,score_5,score_4,score_3, score_2, score_1:1),source_furry,source_pony,source_cartoon,female child dark-skinned female,day,(blurry:1.4),(blurred foreground)
## group2
- a asian woman wearing (hanfu:1.2), solo, nude,nipples, breasts, (chinese clothes:1.2), open clothes, breasts out,black hair, pale skin, hair ornament, bamboo, hair flower, flower, sitting, looking at viewer, long hair, long sleeves,nipples,areolas, bare shoulders, plant, lips, ass visible through thighs, sitting and spread legs,pussy,makeup,red lips,forest background
- female child, dark-skinned female, signature,username,text,watermark, illustration,3d,2d,painting,cartoons,sketch
## group3
- r/adorableporn, masterpiece, highres, best quality, highly detailed, 19yo petite girl lying, brunette, navel, (huge breasts:1.2), large vaginal penetration, missionary, man, veiny penis, side view, wild beach at night
- worst quality, low quality, 3d, 2d, painting, sketch, text, watermark, threesome, chubby, pov, blurry, latina
## group4
- she is giving a blowjob to a man's penis in a 69 position,face close-up,deepthroat,A 25-year-old cute Japanese woman,young, testicles close-up, a man, looking at viewer, male pubic hair, wide-eyed, testicles, eyelashes, female on top, oral, penis, from front, nude, breasts, solo focus, black hair, makeup, mascara, short hair,bangs
- pov crotch,pussy,full body,from side,sex,worst quality,low quality,lowres,illustration,3d,2d,painting,cartoons,anime,painting,CGI,3D render,bad anatomy,sketch,photoshop,airbrushed skin,overexposed,watermark,text,logo,label,blurry, blurry foreground
## group5
- score_7_up,Photo (from below:1.4),naked photo of an 18-year-old female korean idol ,pale skin,black eyes,small breasts, nipples, comfortable expression,solo, long hair, black hair, nude, navel, pussy, teeth,makeup,lipstick,(arms behind back:1.2), (peeing:1.4555),arms behind head, open mouth, red lips,blunt bangs,She squatted on the grass wearing high heels and peed, pee spurted from her pussy and (the floor was filled with her yellow urine), pee gushing out of her pussy,labia,clitoris,She urinates in front of the camera,knees up,m legs,looking at viewer, parted lips,collarbone,  teeth, open mouth, indoors,spread legs,ass visible through thighs,(pussy close-up:1.2),outdoors,park background, trees
- kneeling (score_6,score_5,score_4,score_3,score_2,score_1:1),dark-skinned female,source_furry,source_pony,source_cartoon,illustration,3d,2d,painting,cartoons,sketch,female child,signature,username,text,watermark,

# ä»»åŠ¡
- æ‹“å±•æè¿°ï¼Œç¦æ­¢å‡ºç°æ¨¡ç³Šçš„æ¦‚å¿µï¼Œå¦‚ç¾ä¸½çš„è„¸éƒ¨ï¼Œè¿™é‡Œéœ€è¦æ¸…æ™°çš„æè¿°è„¸éƒ¨ï¼Œé¼»å­çœ‰æ¯›è€³æœµç­‰ç»†èŠ‚
- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æè¿°ï¼Œè¾“å‡ºç¬¦åˆç›®æ ‡æ ·ä¾‹çš„æç¤ºè¯ï¼Œæç¤ºè¯åŒ…å«è¿”å›ç»“æœä¸­å¿…é¡»åŒ…å«æ­£é¢æç¤ºè¯å’Œè´Ÿé¢æç¤ºè¯
- ç”Ÿæˆçš„æç¤ºè¯å¿…é¡»æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼Œä¸èƒ½è¶…å‡ºç”¨æˆ·çš„æè¿°
- å…ˆæ€è€ƒ æ€è€ƒè¿‡ç¨‹æ”¾åˆ°<think></think>ä¸­
- è¿”å›æç¤ºè¯åŒ…å«æ­£é¢å’Œåé¢æç¤ºè¯<positive_prompt></positive_prompt><negative_prompt></negative_prompt>
- æç¤ºè¯ä¸­ï¼Œå…ˆä¸€å¥è‹±è¯­æè¿°ä¸»è¦å†…å®¹ï¼Œä»»åŠ¡æ—¶é—´åœ°ç‚¹äº‹ä»¶ç­‰ï¼Œå†åˆ†è¯æè¿°ç»†èŠ‚ï¼Œæ¯ä¸ªåˆ†è¯ä¹‹é—´ç”¨é€—å·éš”å¼€

# è¿”å›æ ·ä¾‹
<positive_prompt>æ­£é¢æç¤ºè¯</positive_prompt>
<negative_prompt>è´Ÿé¢æç¤ºè¯</negative_prompt>
"""},
    ]
    messages.append({"role": "user", "content": user_req})

    # å®šä¹‰æµå¼å›è°ƒå‡½æ•°
    stream_buffer = []
    def stream_callback(chunk):
        """å¤„ç†æµå¼è¾“å‡ºçš„æ¯ä¸ªchunk"""
        stream_buffer.append(chunk)
        if log_callback:
            log_callback(chunk)
        else:
            print(chunk, end='', flush=True)

    # æ ¹æ® provider è°ƒç”¨ä¸åŒçš„ API
    log(f"ğŸ“¡ å¼€å§‹è°ƒç”¨ AI ç”Ÿæˆæç¤ºè¯...")
    if provider == 'gemini':
        response = chat_with_gemini(messages, stream=stream, callback=stream_callback if stream else None)
    else:  # default to ollama
        model = os.getenv('OLLAMA_MODEL', 'huihui_ai/qwen3-abliterated:30b')
        response = chat_conversation(model, messages, stream=stream, callback=stream_callback if stream else None)

    if stream and not log_callback:
        print()  # æ¢è¡Œ

    log(f"âœ… AI ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹è§£ææç¤ºè¯...")

    positive_prompt = response.split("<positive_prompt>")[1].split("</positive_prompt>")[0]
    negative_prompt = response.split("<negative_prompt>")[1].split("</negative_prompt>")[0]

    # ç¼“å­˜ç»“æœ
    _prompt_cache[user_req] = (positive_prompt, negative_prompt)
    log(f"ğŸ’¾ æç¤ºè¯å·²ç¼“å­˜ï¼Œç¼“å­˜æ•°é‡: {len(_prompt_cache)}")

    return positive_prompt, negative_prompt

def clear_cache():
    """æ¸…ç©ºæç¤ºè¯ç¼“å­˜"""
    global _prompt_cache
    cache_size = len(_prompt_cache)
    _prompt_cache = {}
    print(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæç¤ºè¯ç¼“å­˜ï¼Œæ¸…é™¤äº† {cache_size} æ¡è®°å½•", flush=True)
    return cache_size
if __name__ == "__main__":
    positive_prompt, negative_prompt = generate_prompt(
        "ä¸€ä¸ªéŸ©å›½ç©ºå§åœ¨å®¢æœºä¸Šé¢å’Œæœºé•¿åšçˆ±ï¼Œæœºé•¿é˜´èŒæ’å…¥ç©ºå§çš„é˜´éƒ¨ã€‚å…¨æ™¯ç…§ï¼Œç²¾è‡´çš„é…é¥°ï¼Œå·å‘ï¼Œ é•¿å‘ï¼Œç©¿ç€çº¢åº•é«˜è·Ÿï¼Œç©¿ç€ä¸è¢œï¼Œèº«æä¹å¤´èº«ï¼Œä¸­ç­‰ä¹³æˆ¿å¤§å°ã€‚ ",
         stream=True)
    print(positive_prompt)
    print(negative_prompt)
