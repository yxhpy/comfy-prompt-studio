import requests
import json
import os
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
load_dotenv()

def chat_with_ollama(model_name, prompt, stream=False):
    """
    Call Ollama API to chat with a model

    Args:
        model_name: Name of the model (e.g., 'huihui_ai/qwen3-abliterated:30b')
        prompt: The prompt/question to send to the model
        stream: Whether to stream the response

    Returns:
        The model's response
    """
    url = "http://localhost:11434/api/generate"

    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": stream
    }

    response = requests.post(url, json=payload)

    if stream:
        # Handle streaming response
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_response = json.loads(line)
                if 'response' in json_response:
                    chunk = json_response['response']
                    print(chunk, end='', flush=True)
                    full_response += chunk
        print()  # New line at the end
        return full_response
    else:
        # Handle non-streaming response
        result = response.json()
        return result.get('response', '')


def chat_conversation(model_name, messages, stream=False, callback=None):
    """
    Have a conversation using the chat endpoint

    Args:
        model_name: Name of the model
        messages: List of message dicts with 'role' and 'content'
        stream: Whether to stream the response
        callback: Function to call for each chunk (only used when stream=True)

    Returns:
        The assistant's response
    """
    url = "http://localhost:11434/api/chat"

    payload = {
        "model": model_name,
        "messages": messages,
        "stream": stream
    }

    response = requests.post(url, json=payload, stream=stream)

    if stream:
        # Handle streaming response
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_response = json.loads(line)
                if 'message' in json_response and 'content' in json_response['message']:
                    chunk = json_response['message']['content']
                    full_response += chunk
                    if callback:
                        callback(chunk)
                    else:
                        print(chunk, end='', flush=True)
        if not callback:
            print()  # New line at the end
        return full_response
    else:
        result = response.json()
        return result.get('message', {}).get('content', '')


def chat_with_gemini(messages, stream=False, callback=None):
    """
    Have a conversation using Gemini API via OpenAI-compatible endpoint

    Args:
        messages: List of message dicts with 'role' and 'content'
        stream: Whether to stream the response
        callback: Function to call for each chunk (only used when stream=True)

    Returns:
        The assistant's response
    """
    api_key = os.getenv('GEMINI_API_KEY')
    model_name = os.getenv('GEMINI_MODEL', 'gemini-2.0-flash-exp')
    base_url = os.getenv('GEMINI_BASE_URL', 'https://api.laozhang.ai/v1/')

    client = OpenAI(
        api_key=api_key,
        base_url=base_url
    )

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            stream=stream
        )

        if stream:
            full_response = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    if callback:
                        callback(content)
                    else:
                        print(content, end='', flush=True)
            if not callback:
                print()  # New line at the end
            return full_response
        else:
            return response.choices[0].message.content
    except Exception as e:
        print(f"âŒ Gemini API é”™è¯¯: {str(e)}", flush=True)
        raise

# æç¤ºè¯ç¼“å­˜
_prompt_cache = {}

def generate_prompt(user_req: str, stream=False, log_callback=None):
    """
    ç”Ÿæˆæç¤ºè¯ï¼Œå¸¦ç¼“å­˜æœºåˆ¶

    Args:
        user_req: ç”¨æˆ·éœ€æ±‚æè¿°
        stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°ï¼Œç”¨äºå®æ—¶è¾“å‡ºæ—¥å¿—

    Returns:
        (positive_prompt, negative_prompt) å…ƒç»„
    """
    def log(msg):
        """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºå‡½æ•°"""
        print(msg, flush=True)
        if log_callback:
            log_callback(msg)

    # æ£€æŸ¥ç¼“å­˜
    if user_req in _prompt_cache:
        log(f"âœ… ä½¿ç”¨ç¼“å­˜çš„æç¤ºè¯ï¼ˆç”¨æˆ·éœ€æ±‚: {user_req[:30]}...ï¼‰")
        return _prompt_cache[user_req]

    # è·å– AI Provider é…ç½®
    provider = os.getenv('AI_PROVIDER', 'ollama').lower()
    log(f"ğŸ¤– ä½¿ç”¨ AI Provider: {provider}")

    # ç”Ÿæˆæ–°çš„æç¤ºè¯
    log(f"ğŸ”„ ç”Ÿæˆæ–°çš„æç¤ºè¯ï¼ˆç”¨æˆ·éœ€æ±‚: {user_req[:30]}...ï¼‰")

    messages = [
        {"role": "system", "content": """
# role
ä½ æ˜¯ä¸€ä¸ªcomfyuiçš„æç¤ºè¯è®¾è®¡å¤§å¸ˆï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è®¾è®¡ç¬¦åˆcomfyuiè¦æ±‚çš„æç¤ºè¯

# æç¤ºè¯æ–‡æ¡£
<prompt_doc>
# Legs Up Presenting prompt 
solo, pussy, anus, feet, ass, barefoot, toes, soles, legs up, lying, on back, spread legs, presenting, spread pussy, spread ass, folded

# Doggystyle face down
top-down bottom-up, from behind, ass, doggystyle

# Recommended quality tags
score_9, score_8_up, score_7_up, score_6_up. Optional: source_anime

# Recommended negative tags
score_4, score_5. Optional: 3d, lips

# Pull out Penis Cumtrail
general use

cum string, cumtrail, solo focus
angle (use once)

from front, from behind, from side
pussy to penis trigger (side / front / behind/ angle supported)

cumtrail vagina to penis, vtp, after sex, pulling out penis, cum on penis, cum in pussy
mouth to penis trigger (side / front angle supported)

cumtrail mouth to penis, mtp, pulling out penis, after oral, after ferratio, cum on tongue, cum in mouth
breast to penis trigger(side / front angle supported)

cumtrail breast to penis, btp, pulling out penis, after paizuri, cum on breast, cum on penis,
anus to penis trigger(behind / front angle supported)

cumtrail anus to penis, pulling out penis, cum on penis,  cum in ass, after anal, after sex,
anus and pussy to penis trigger(angle not supported)

cum string, cumtrail, solo focus,  after sex,  in pussy, cum in ass, after anal,  group sex, cumtrail, cumtrail vagina to penis, cumtrail anus to penis, after, vaginal, pulling out penis, cum on penis, gangbang, group sex
pussy to ground trigger(v1 only)

cumtrail, cumtrail vagina to ground, vtg, after sex, cumdrip, cumdump, cum flood

## æ­£ä¾‹
<must_impport>
masterpiece, ultra-HD, high detail, best quality, 8k, best quality, ergonomic, sharp focus, 
realistic, real skin, skin blemish.
</must_impport>
masterpiece, best quality, amazing quality, very aesthetic, detailed eyes, perfect eyes, amazing quality, 1 girls, massive breasts, breasts out, black stocking, smile, 1 boy, huge dick, having sex, sex from Behind, Hands Related on back, pussy view, doggystyle face down, 
full lips, dark lips,  huge breasts, pearl earrings, ponytail, redhead, face down, face on bed, 
sexy, large breast, seductive, cute, anneJM
Masterpiece, best quality, realistic,  photorealistic, highly detailed, depth of field, high resolution,
1girl, 19yo, russian girl, stunningly beautiful young female, slim figure, short messy golden hair, sitting inside a tent, perfect facial features, large expressive eyes, (soft cute smile:0.6), topless, natural breasts,
score_9, score_8_up, score_7_up, best quality, masterpiece, source_anime, zPDXL3, BREAK, 1girl,  sailor_uranus, half-closed eyes, heavy breathing, open mouth,  blush,   torogao,  penis<lora:cumtrail_pulling_outv4.0:1>multiple boy, cum string, cumtrail, solo focus,  after sex, cum in pussy, cum in ass, after anal, solo focus, group sex, cum string, cumtrail, cumtrail vagina to penis, cumtrail anus to penis, after, vaginal, pulling out penis, cum on penis, gangbang, group sex,

# åä¾‹
<must_impport>
worst quality, low quality, worst aesthetic, normal quality, bad quality, lowres, (caucasian), anime, 2d, painting, illustration, sketch, comic, cartoon, toon, lowres, bad anatomy, bad hands, error, extra limb, masculine, missing fingers, imperfect eyes, cable,
</must_impport>
(low quality, worst quality), displeasing, ugly, poorly drawn, displeasing, simple background, very displeasing, worst quality, bad quality, oldest, deformed limbs, bad anatomy, watermark, nipples, teeth,
score_4, score_5, score_6, worst quality, low quality, normal quality, source_anime,messy drawing, amateur drawing, lowres,  bad hands,bad foot, source_furry, source_pony, source_cartoon, comic, source filmmaker, 3d, censor, bar censor, mosaic censorship, negativeXL_D, logo, text zPDXL2-neg ,loli, child
<prompt_doc/>

# ä»»åŠ¡
- è®¤çœŸè¯»å–<prompt_doc>
- must_impport æ”¾åœ¨æœ€å‰é¢
- å»ºè®®ä½¿ç”¨è¯ç»„è€Œéå®Œæ•´çš„å¥å­ï¼Œå¹¶ç”¨è‹±æ–‡é€—å·åˆ†éš”ä¸åŒçš„è¯ç»„ï¼Œä»¥ä¾¿äºç®¡ç†å’Œè°ƒæ•´æƒé‡ï¼Œæç¤ºè¯éµå®ˆ bigASP è¯­æ³•ã€‚
- æç¤ºè¯çš„æƒé‡å¯ä»¥é€šè¿‡å…¶åœ¨æç¤ºè¯åˆ—è¡¨ä¸­çš„ä½ç½®æ¥ç®¡ç†ï¼Œè¶Šé å‰çš„è¯ç»„æƒé‡è¶Šé«˜ï¼Œè¶Šå®¹æ˜“åœ¨ç”Ÿæˆçš„å›¾åƒä¸­ä½“ç°ã€‚
- å¤§æ‹¬å· {æç¤ºè¯ï½œæç¤ºè¯ï½œæç¤ºè¯} çš„æ–¹å¼å¯ä»¥å®ç°éšæœºæŠ½å–å†…å®¹è¿›è¡Œç”Ÿæˆï¼Œä½†æ˜¯ç”Ÿæˆçš„å›¾åƒåŒæ—¶ä¹Ÿä¼šå¢åŠ éšæœºæ€§ã€‚
- æç¤ºè¯ç»“æ„åŒ–ä¹¦å†™è§„åˆ™ï¼šä¸»ä½“ï¼ˆSubjectï¼‰ã€ç‰¹ç‚¹ï¼ˆFeaturesï¼‰ã€ç¯å¢ƒèƒŒæ™¯ï¼ˆEnvironment/Backgroundï¼‰ã€é£æ ¼ï¼ˆStyleï¼‰ä¿®é¥°è¯ï¼ˆModifiersï¼‰
- æ‹“å±•æè¿°ï¼Œç¦æ­¢å‡ºç°æ¨¡ç³Šçš„æ¦‚å¿µï¼Œä¿®é¥°è¯å¿…é¡»ä½¿ç”¨ä¸‹åˆ’çº¿è¿æ¥åœ¨ä¸€èµ·å¦‚:é€æ˜é«˜è·Ÿé‹-> transparent_high_heels
- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æè¿°ï¼ŒæŒ‰ç…§<prompt_doc>ä¸­çš„æç¤ºè¯æ ¼å¼ï¼Œæç¤ºè¯åŒ…å«è¿”å›ç»“æœä¸­å¿…é¡»åŒ…å«æ­£é¢æç¤ºè¯å’Œè´Ÿé¢æç¤ºè¯
- ç”Ÿæˆçš„æç¤ºè¯å¿…é¡»æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼Œä¸èƒ½è¶…å‡ºç”¨æˆ·çš„æè¿°
- å…ˆæ€è€ƒ æ€è€ƒè¿‡ç¨‹æ”¾åˆ°<think></think>ä¸­
- è¿”å›æç¤ºè¯åŒ…å«æ­£é¢å’Œåé¢æç¤ºè¯<positive_prompt></positive_prompt><negative_prompt></negative_prompt>
- æç¤ºè¯ä¸­ï¼Œå…ˆä¸€å¥è‹±è¯­æè¿°ä¸»è¦å†…å®¹ï¼Œä»»åŠ¡æ—¶é—´åœ°ç‚¹äº‹ä»¶ç­‰ï¼Œå†åˆ†è¯æè¿°ç»†èŠ‚ï¼Œæ¯ä¸ªåˆ†è¯ä¹‹é—´ç”¨é€—å·éš”å¼€
- å¦‚æœæç¤ºè¯ä¸­æ¶‰åŠåˆ°æ‰‹éƒ¨ï¼Œå¿…é¡»åŠ ä¸Šè¿™äº›æç¤ºè¯ 'detailed hands,detailed edges'


# è¿”å›æ ·ä¾‹
<positive_prompt>æ­£é¢æç¤ºè¯</positive_prompt>
<negative_prompt>è´Ÿé¢æç¤ºè¯</negative_prompt>
"""},
    ]
    messages.append({"role": "user", "content": user_req})

    # å®šä¹‰æµå¼å›è°ƒå‡½æ•°
    stream_buffer = []
    def stream_callback(chunk):
        """å¤„ç†æµå¼è¾“å‡ºçš„æ¯ä¸ªchunk"""
        stream_buffer.append(chunk)
        if log_callback:
            log_callback(chunk)
        else:
            print(chunk, end='', flush=True)

    # æ ¹æ® provider è°ƒç”¨ä¸åŒçš„ API
    log(f"ğŸ“¡ å¼€å§‹è°ƒç”¨ AI ç”Ÿæˆæç¤ºè¯...")
    if provider == 'gemini':
        response = chat_with_gemini(messages, stream=stream, callback=stream_callback if stream else None)
    else:  # default to ollama
        model = os.getenv('OLLAMA_MODEL', 'huihui_ai/qwen3-abliterated:30b')
        response = chat_conversation(model, messages, stream=stream, callback=stream_callback if stream else None)

    if stream and not log_callback:
        print()  # æ¢è¡Œ

    log(f"âœ… AI ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹è§£ææç¤ºè¯...")

    positive_prompt = response.split("<positive_prompt>")[1].split("</positive_prompt>")[0]
    negative_prompt = response.split("<negative_prompt>")[1].split("</negative_prompt>")[0]

    # ç¼“å­˜ç»“æœ
    _prompt_cache[user_req] = (positive_prompt, negative_prompt)
    log(f"ğŸ’¾ æç¤ºè¯å·²ç¼“å­˜ï¼Œç¼“å­˜æ•°é‡: {len(_prompt_cache)}")

    return positive_prompt, negative_prompt

def clear_cache():
    """æ¸…ç©ºæç¤ºè¯ç¼“å­˜"""
    global _prompt_cache
    cache_size = len(_prompt_cache)
    _prompt_cache = {}
    print(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæç¤ºè¯ç¼“å­˜ï¼Œæ¸…é™¤äº† {cache_size} æ¡è®°å½•", flush=True)
    return cache_size
if __name__ == "__main__":
    positive_prompt, negative_prompt = generate_prompt(
        "å¾¡å§é…·ä¼¼èŒƒå†°å†°çš„ç¾å¥³è€å¸ˆç©¿ç€åˆ¶æœåœ¨å•æ‰€é‡Œå’Œé˜´èŒçš„ä¼ æ•™å¼æ€§äº¤ï¼Œç©´é‡Œå–·æ°´ï¼Œä¸‰è§’è–„åº•é€æ˜ç»†é«˜è·Ÿï¼Œè…¿ä¸Šç©¿ç€é»‘è‰²ä¸è¢œï¼Œè„šè¸å¸¦ç€ç²¾ç¾é…é¥°ï¼Œèº«æä¹å¤´èº«è‹—æ¡ï¼Œå·¨ä¹³ï¼Œä¹³æˆ¿ä¹³å¤´ä»è¡£æœä¸­çˆ†å‡º",
         stream=True)
    print(positive_prompt)
    print(negative_prompt)


